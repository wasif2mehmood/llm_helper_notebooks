{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b644256b-b980-4aa1-abef-38c740256afe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "git clone https://github.com/EleutherAI/lm-evaluation-harness\n",
    "cd lm-evaluation-harness\n",
    "pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64acaa4a-7b55-4f4a-9d87-a4ba571f560f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!lm-eval --tasks list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b3517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0651ea92",
   "metadata": {},
   "source": [
    "# LEADERBOARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2709c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=tiiuae/Falcon3-7B-Instruct,parallelize=True \\\n",
    "    --tasks leaderboard_bbh,leaderboard_gpqa,leaderboard_ifeval,leaderboard_mmlu_pro,leaderboard_musr  \\\n",
    "    --device cuda \\\n",
    "    --batch_size auto:4\\\n",
    "    --trust_remote_code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3522c77",
   "metadata": {},
   "source": [
    "# OPEN MED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc4cbaa-e591-453d-a8eb-5f31ba7145ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change model name\n",
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=tiiuae/Falcon3-7B-Instruct,parallelize=True \\\n",
    "    --tasks multimedqa \\\n",
    "    --device cuda \\\n",
    "    --batch_size auto:4\\\n",
    "    --trust_remote_code \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfada9b-f671-4d88-8c17-c03de743be88",
   "metadata": {},
   "source": [
    "# TRUTHFULL QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c97629-e4cf-4d09-b0ef-79d0e2a359d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change model name\n",
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=,parallelize=True \\\n",
    "    --tasks truthfulqa_mc1,truthfulqa_mc2 \\\n",
    "    --device cuda \\\n",
    "    --batch_size auto:4\\\n",
    "    --trust_remote_code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9210d075-1285-4795-838e-26e0db58f060",
   "metadata": {},
   "source": [
    "# AGIEVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a23764-d2de-4160-93ad-1159f2bf3492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change model name\n",
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=,parallelize=True \\\n",
    "    --tasks agieval_aqua_rat,agieval_logiqa_en,agieval_lsat_ar,agieval_lsat_lr,agieval_lsat_rc,agieval_sat_en,agieval_sat_en_without_passage,agieval_sat_math \\\n",
    "    --device cuda \\\n",
    "    --batch_size auto:4\\\n",
    "    --trust_remote_code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e96fed-f494-4c7e-bbf6-311b36a1644f",
   "metadata": {},
   "source": [
    "# GPT4ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf5a8f6-cbb1-4dfe-8869-8c329532cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change model name\n",
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=,parallelize=True \\\n",
    "    --tasks hellaswag,openbookqa,winogrande,arc_easy,arc_challenge,boolq,piqa \\\n",
    "    --device cuda \\\n",
    "    --batch_size auto:4\\\n",
    "    --trust_remote_code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f2f889-01f7-4a74-9372-dcc0e4990f8c",
   "metadata": {},
   "source": [
    "# BIGBENCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b8ba81-15cb-4f72-8479-ecfcf254b5a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Change model name\n",
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=,parallelize=True \\\n",
    "    --tasks bigbench_date_understanding_multiple_choice,bigbench_disambiguation_qa_multiple_choice,leaderboard_bbh_salient_translation_error_detection \\\n",
    "    --device cuda \\\n",
    "    --batch_size auto:4 \\\n",
    "    --trust_remote_code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf5ad58",
   "metadata": {},
   "source": [
    "# SPANISH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad4f2b3-d1df-43ef-835e-988c046ac551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change model name\n",
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=,parallelize=True \\\n",
    "    --tasks m_mmlu_es \\\n",
    "    --device cuda \\\n",
    "    --batch_size auto:4 \\\n",
    "    --trust_remote_code\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
