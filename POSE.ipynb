{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Caf-bZln7gvh"
      },
      "outputs": [],
      "source": [
        "MODEL = \"LLama-3-8B-32k\"\n",
        "yaml_config = f\"\"\"\n",
        "base_model: meta-llama/Meta-Llama-3-8B\n",
        "model_type: LlamaForCausalLM\n",
        "tokenizer_type: AutoTokenizer\n",
        "\n",
        "load_in_8bit: true\n",
        "load_in_4bit: false\n",
        "strict: false\n",
        "\n",
        "datasets:\n",
        "  - path: togethercomputer/RedPajama-Data-1T-Sample\n",
        "    type: completion\n",
        "    split: train\n",
        "dataset_prepared_path: last_run_prepared\n",
        "val_set_size: 0.001\n",
        "output_dir: ./llama-3-32k\n",
        "save_safetensors: true\n",
        "sequence_len: 8192\n",
        "sample_packing: false\n",
        "pad_to_sequence_len: false\n",
        "use_pose: true\n",
        "pose_max_context_len: 65536\n",
        "\n",
        "overrides_of_model_config:\n",
        "  rope_theta: 500000.0\n",
        "  max_position_embeddings: 65536\n",
        "\n",
        "  # peft_use_dora: true\n",
        "adapter: lora\n",
        "peft_use_rslora: true\n",
        "lora_model_dir:\n",
        "lora_r: 256\n",
        "lora_alpha: 256\n",
        "lora_dropout: 0.1\n",
        "lora_target_modules:\n",
        "  - q_proj\n",
        "  - k_proj\n",
        "  - v_proj\n",
        "  - o_proj\n",
        "\n",
        "wandb_project: llama-3-64k\n",
        "wandb_entity: zaiinn440\n",
        "wandb_watch:\n",
        "wandb_name:\n",
        "wandb_log_model:\n",
        "\n",
        "gradient_accumulation_steps: 8\n",
        "micro_batch_size: 1\n",
        "num_epochs: 1\n",
        "optimizer: adamw_bnb_8bit\n",
        "lr_scheduler: cosine\n",
        "learning_rate: 0.00003\n",
        "\n",
        "train_on_inputs: false\n",
        "group_by_length: false\n",
        "bf16: true\n",
        "fp16:\n",
        "tf32: true\n",
        "\n",
        "gradient_checkpointing: true\n",
        "gradient_checkpointing_kwargs:\n",
        "  use_reentrant: true\n",
        "early_stopping_patience:\n",
        "resume_from_checkpoint:\n",
        "local_rank:\n",
        "logging_steps: 1\n",
        "xformers_attention:\n",
        "flash_attention: true\n",
        "sdp_attention:\n",
        "s2_attention:\n",
        "\n",
        "warmup_steps: 10\n",
        "evals_per_epoch: 8\n",
        "saves_per_epoch: 8\n",
        "debug:\n",
        "deepspeed:\n",
        "weight_decay: 0.0\n",
        "fsdp:\n",
        "fsdp_config:\n",
        "special_tokens:\n",
        "  pad_token: <|end_of_text|>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1z4cM9er_cS8"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import runpod\n",
        "except ImportError:\n",
        "  !pip install -qqq runpod --progress-bar off\n",
        "\n",
        "import runpod\n",
        "from runpod.error import QueryError\n",
        "from google.colab import userdata, runtime\n",
        "import requests\n",
        "import json\n",
        "import yaml\n",
        "import requests\n",
        "import time\n",
        "\n",
        "def upload_gist(text, gist_name, gh_token, gist_description=\"\"):\n",
        "    gist_content = {\n",
        "        \"description\": gist_description,\n",
        "        \"public\": False,\n",
        "        \"files\": {\n",
        "            f\"{gist_name}\": {\n",
        "                \"content\": text\n",
        "            }\n",
        "        },\n",
        "    }\n",
        "\n",
        "    # Headers for the request\n",
        "    headers = {\n",
        "        \"Authorization\": f\"token {gh_token}\",\n",
        "        \"Accept\": \"application/vnd.github.v3+json\",\n",
        "    }\n",
        "\n",
        "    # Make the request\n",
        "    response = requests.post(\n",
        "        \"https://api.github.com/gists\", headers=headers, json=gist_content\n",
        "    )\n",
        "\n",
        "    if response.status_code == 201:\n",
        "        gist_data = response.json()\n",
        "        raw_url = gist_data['files'][gist_name]['raw_url']\n",
        "        print(f\"Uploaded Axolotl config as gist: {raw_url}\")\n",
        "        return raw_url\n",
        "    else:\n",
        "        print(\n",
        "            f\"Failed to upload gist. Status code: {response.status_code}. Response: {response.text}\"\n",
        "        )\n",
        "        return None\n",
        "\n",
        "def delete_gist(gh_token, gist_id):\n",
        "    # Headers for the request\n",
        "    headers = {\n",
        "        \"Authorization\": f\"token {gh_token}\",\n",
        "        \"Accept\": \"application/vnd.github.v3+json\",\n",
        "    }\n",
        "\n",
        "    # Make the request\n",
        "    response = requests.delete(\n",
        "        f\"https://api.github.com/gists/{gist_id}\", headers=headers\n",
        "    )\n",
        "\n",
        "    if response.status_code == 204:\n",
        "        print(\"Gist has been deleted.\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\n",
        "            f\"Failed to delete gist. Status code: {response.status_code}. Response: {response.text}\"\n",
        "        )\n",
        "        return False\n",
        "\n",
        "\n",
        "GPU = \"NVIDIA RTX 6000 Ada Generation\"\n",
        "NUMBER_OF_GPUS = 8\n",
        "CONTAINER_DISK = 300\n",
        "CLOUD_TYPE = \"SECURE\"\n",
        "SCRIPT = \"https://gist.githubusercontent.com/mlabonne/160015f39c05b92cea83a57d93d552fe/raw\"\n",
        "ZERO = \"None\"\n",
        "LLM_AUTOEVAL = False\n",
        "DEBUG = False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "USERNAME = \"\"\n",
        "RUNPOD_TOKEN = \"\"\n",
        "HUGGING_FACE_TOKEN = \"HF_TOKEN\"\n",
        "WANDB_TOKEN = \"wandb\"\n",
        "GITHUB_TOKEN = \"github\"\n",
        "\n",
        "\n",
        "runpod.api_key = RUNPOD_TOKEN\n",
        "WANDB_API_KEY = WANDB_TOKEN\n",
        "HF_TOKEN = HUGGING_FACE_TOKEN\n",
        "GITHUB_API_TOKEN = GITHUB_TOKEN\n",
        "\n",
        "# Make sure it's a valid YAML file\n",
        "config = yaml.safe_load(yaml_config)\n",
        "\n",
        "# Upload the YAML file to GitHub\n",
        "gist_url = upload_gist(yaml_config, \"config.yaml\", GITHUB_API_TOKEN, f\"{MODEL} - https://huggingface.co/{USERNAME}/{MODEL}\")\n",
        "\n",
        "# Summary\n",
        "base_model = config.get('base_model', 'Unknown model')\n",
        "dataset_info = []\n",
        "datasets = config.get('datasets', [])\n",
        "for dataset in datasets:\n",
        "    path = dataset.get('path', 'Unknown path')\n",
        "    dtype = dataset.get('type', 'Unknown type')\n",
        "    dataset_info.append(f\"{path} ({dtype})\")\n",
        "datasets_summary = ', '.join(dataset_info)\n",
        "\n",
        "# Create a pod\n",
        "keep_trying = True\n",
        "try:\n",
        "    while keep_trying:\n",
        "        try:\n",
        "            pod = runpod.create_pod(\n",
        "                name=f\"LazyAxolotl - {MODEL}\",\n",
        "                image_name=\"winglian/axolotl-cloud:main-latest\",\n",
        "                gpu_type_id=GPU,\n",
        "                cloud_type=CLOUD_TYPE,\n",
        "                gpu_count=NUMBER_OF_GPUS,\n",
        "                volume_in_gb=0,\n",
        "                container_disk_in_gb=CONTAINER_DISK,\n",
        "                template_id=\"eul6o46pab\",\n",
        "                env={\n",
        "                    \"HF_TOKEN\": HF_TOKEN,\n",
        "                    \"SCRIPT\": SCRIPT,\n",
        "                    \"WANDB_API_KEY\": WANDB_API_KEY,\n",
        "                    \"GIST_URL\": gist_url,\n",
        "                    \"MODEL_NAME\": MODEL,\n",
        "                    \"BASE_MODEL\": config['base_model'],\n",
        "                    \"USERNAME\": USERNAME,\n",
        "                    \"ZERO\": ZERO,\n",
        "                    \"LLM_AUTOEVAL\": LLM_AUTOEVAL,\n",
        "                    \"MODEL_ID\": USERNAME + \"/\" + MODEL,\n",
        "                    \"BENCHMARK\": \"nous\",\n",
        "                    \"REPO\": \"https://github.com/mlabonne/llm-autoeval.git\",\n",
        "                    \"TRUST_REMOTE_CODE\": True,\n",
        "                    \"GITHUB_API_TOKEN\": GITHUB_API_TOKEN,\n",
        "                    \"DEBUG\": DEBUG,\n",
        "                }\n",
        "            )\n",
        "            print(f\"This runs trains {base_model} on {datasets_summary}.\")\n",
        "            print(\"https://www.runpod.io/console/pods\")\n",
        "            keep_trying = False\n",
        "        except QueryError as e:\n",
        "            print(f\"\\033[31m⚠️ ERROR: The requested pod ({NUMBER_OF_GPUS}x {GPU} with {CONTAINER_DISK} GB) is not currently available. Trying again in 30 seconds...\\033[39m\")\n",
        "            time.sleep(30)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"KeyboardInterrupt detected, cleaning up before stopping...\")\n",
        "    delete_gist(GITHUB_API_TOKEN, gist_url.split('/')[4])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
