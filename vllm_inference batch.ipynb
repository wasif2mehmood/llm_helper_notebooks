{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ae422b-48ba-4741-aac8-47804cde0e51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install vllm kaleido python-multipart typing-extensions torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0216b5aa-3815-40d1-b82b-6a9758a8a300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f2260f-2170-44cc-9261-d2dfa75646c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token ''  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cfc486-759a-43b6-bf6d-c03b1a1d9464",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets tqdm openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feec6000-7e4b-46b8-a4de-9c4de269143f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "from vllm import SamplingParams, LLM\n",
    "\n",
    "# Load the local Excel file\n",
    "df = pd.read_excel(\"input.xlsx\")\n",
    "\n",
    "# Concatenate the \"prompt\" and \"text\" columns\n",
    "df['combined'] = df['prompt'].astype(str) + \" \" + df['text'].astype(str)\n",
    "\n",
    "# Convert the DataFrame to a Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Use the complete dataset\n",
    "prompts = dataset['combined']\n",
    "\n",
    "# Define sampling parameters and LLM model\n",
    "sampling_params = SamplingParams(max_tokens=1000)\n",
    "llm = LLM(model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\", tensor_parallel_size=4)\n",
    "\n",
    "def generate_batch(prompts):\n",
    "    # Ensure prompts are in the correct format\n",
    "    if not isinstance(prompts, list):\n",
    "        prompts = [prompts]\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    return [output.outputs[0].text for output in outputs]\n",
    "\n",
    "# Check if the output file exists and determine the starting index\n",
    "output_file = \"generated_responses.xlsx\"\n",
    "if os.path.exists(output_file):\n",
    "    existing_df = pd.read_excel(output_file)\n",
    "    start_index = existing_df['response'].last_valid_index() + 1\n",
    "    generated_text = existing_df['response'].tolist()\n",
    "else:\n",
    "    start_index = 0\n",
    "    generated_text = []\n",
    "\n",
    "# Split prompts into batches starting from the determined index\n",
    "batch_size = 4  # Adjust batch size as needed\n",
    "batches = [prompts[i:i + batch_size] for i in range(start_index, len(prompts), batch_size)]\n",
    "\n",
    "time_taken = 0\n",
    "\n",
    "# Generate text in batches\n",
    "for batch_index, batch in enumerate(tqdm(batches, initial=start_index // batch_size, total=len(prompts) // batch_size)):\n",
    "    start = time.time()\n",
    "    batch_results = generate_batch(batch)\n",
    "    taken = time.time() - start\n",
    "    time_taken += taken\n",
    "    \n",
    "    # Print responses after each batch\n",
    "    for response in batch_results:\n",
    "        print(response)\n",
    "    \n",
    "    generated_text.extend(batch_results)\n",
    "    \n",
    "    # Save responses to an Excel file after each batch\n",
    "    response_df = pd.DataFrame({'prompt': prompts[:len(generated_text)], 'response': generated_text})\n",
    "    response_df.to_excel(output_file, index=False)\n",
    "\n",
    "# Count tokens in generated text\n",
    "tokens = sum(len(sample.split()) for sample in generated_text)\n",
    "\n",
    "print(tokens)\n",
    "print(\"tok/s\", tokens // time_taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1092fe7-c14d-47e1-9b6c-cda4a6a8b2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
