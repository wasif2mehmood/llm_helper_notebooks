{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers accelerate pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token ''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "# token=\"\"\n",
    "MODEL = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype= torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\n",
    "\n",
    "\n",
    "TERMINATORS = [\n",
    "    TOKENIZER.eos_token_id,\n",
    "    TOKENIZER.convert_tokens_to_ids(\"<|endoftext|>\")\n",
    "]\n",
    "\n",
    "def infer_batch(\n",
    "    list_of_inputs: list,\n",
    "    max_new_tokens: int,\n",
    "    temperature: float,\n",
    "    top_p:float,\n",
    "    top_k:float,\n",
    "    do_sample:bool,\n",
    "    use_cache:bool\n",
    ") -> list[str]:\n",
    "\n",
    "    template_string = [\n",
    "        TOKENIZER.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": example},\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "        for example in list_of_inputs\n",
    "    ]\n",
    "\n",
    "    input_ids = TOKENIZER(template_string, return_tensors=\"pt\",padding=True).to(\n",
    "        MODEL.device\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = MODEL.generate(\n",
    "            **input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,  # Adjusted temperature\n",
    "            top_p=top_p,        # Adjusted top_p\n",
    "            top_k=top_k,         # Adjusted top_k\n",
    "            do_sample=do_sample,\n",
    "            use_cache=use_cache\n",
    "            ,\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    responses = TOKENIZER.batch_decode(\n",
    "        outputs[:, input_ids[\"input_ids\"].shape[-1] :], skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = {\n",
    "    \"summarization\": \"Summarize the following document:\\n## Document Start ##\\n{v}\\n## Document End ##\",\n",
    "    \"open_book_qa\": \"Answer the following question based on the given context:\\n## Context Start ##\\n{context}\\n## Context End ##\\n## Question Start ##\\n{question}\\n## Question End ##\",\n",
    "    \"closed_book_qa\": \"Answer the following question:\\n## Question Start ##\\n{v}\\n## Question End ##\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input_text(input_text, template_type):\n",
    "    if template_type is None:\n",
    "        if isinstance(input_text, list) and all(\n",
    "            isinstance(item, dict) for item in input_text\n",
    "        ):\n",
    "            input_text = [\n",
    "                f\"{item['context']}\\n{item['question']}\" for item in input_text\n",
    "            ]\n",
    "    else:\n",
    "        template = user_prompt[template_type]\n",
    "        if isinstance(input_text, list) and all(\n",
    "            isinstance(item, dict) for item in input_text\n",
    "        ):\n",
    "            input_text = [template.format(**item) for item in input_text]\n",
    "        else:\n",
    "            input_text = [template.format(v=item) for item in input_text]\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset from Hugging Face\n",
    "dataset = load_dataset('ccdv/pubmed-summarization', split='train')\n",
    "\n",
    "# Extract the specific entries from the dataset\n",
    "entry_numbers = [23, 306, 573, 812]\n",
    "batch_entries = [dataset[i]['article'] for i in entry_numbers]\n",
    "\n",
    "# Pass the batch to the process_input_text method\n",
    "process_input = process_input_text(batch_entries, \"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(process_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens =1024\n",
    "temperature=0.7  # Adjusted temperature\n",
    "top_p=0.9       # Adjusted top_p\n",
    "top_k=50         \n",
    "\n",
    "res = infer_batch(process_input, max_new_tokens, temperature,  top_p, top_k,True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, r in enumerate(res):\n",
    "    print(f\"Response {i + 1}: {r}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
