{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQUUWe-wavBY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlqve98SamqR"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"argilla/reward-model-data-falcon\")\n",
        "\n",
        "\n",
        "def create_dataset(row):\n",
        "  if row['choose-best']['value'][0]==2:\n",
        "    row['response-1'],row['response-2']=row['response-2'],row['response-1']\n",
        "  return row\n",
        "\n",
        "dataset=dataset.map(lambda x: create_dataset(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Gn6lxm1a1Lf"
      },
      "outputs": [],
      "source": [
        "def loss_function(preferred_response_reward, alternate_response_reward):\n",
        "    return -torch.mean(torch.log(torch.sigmoid(alternate_response_rewar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpeWn0w6a4uu"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Model, GPT2Tokenizer\n",
        "\n",
        "class GPT2RewardModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.backbone = GPT2Model.from_pretrained('gpt2')\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "        self.regression_head = torch.nn.Linear(768, 1)\n",
        "\n",
        "    def forward(self, context, response):\n",
        "\n",
        "        entire_text = context + response\n",
        "        context_dict = self.tokenizer(\n",
        "            '<|startoftext|>' + entire_text + '<|endoftext|>'\n",
        "        )\n",
        "\n",
        "        input_ids = torch.tensor(context_dict.input_ids)\n",
        "        attention_mask = torch.tensor(context_dict.attention_mask)\n",
        "\n",
        "\n",
        "        gpt2_outputs = self.backbone(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "        all_output_vectors = gpt2_outputs.last_hidden_state\n",
        "        last_output_vector = all_output_vectors[-1]\n",
        "\n",
        "\n",
        "        last_output_vector = last_output_vector.unsqueeze(0)\n",
        "        reward = self.regression_head(last_output_vector)\n",
        "\n",
        "        return reward\n",
        "model = GPT2RewardModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yQPV3DrfXSH"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qje9-1bka7uP"
      },
      "outputs": [],
      "source": [
        "def train(epochs=10):\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=1e-5)\n",
        "\n",
        "\n",
        "    batch_idx = 0\n",
        "    # Train the model\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}\")\n",
        "        for batch in tqdm(dataset):\n",
        "\n",
        "            # Get the data\n",
        "            prompt, preferred_reponse, alternate_response,choose,external_id = batch\n",
        "            preferred_response_reward = model(prompt, preferred_reponse)\n",
        "            alternate_response_reward = model(prompt, alternate_response)\n",
        "\n",
        "            loss = loss_function(preferred_response_reward, alternate_response_reward)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Log the loss\n",
        "            print(f\"Loss: {loss.item()}\", batch_idx)\n",
        "            batch_idx += 1\n",
        "\n",
        "\n",
        "train()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
