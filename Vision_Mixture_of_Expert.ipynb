{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYKk9LoomjZK"
      },
      "outputs": [],
      "source": [
        "!pip install transformers -U\n",
        "!pip install huggingface_hub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BnuCQjMmoVM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoProcessor,AutoConfig\n",
        "\n",
        "def count_parameters(model):\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    #number of parameters in b\n",
        "    return total_params/1e9, trainable_params/1e9\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_name_moe = \"lamm-mit/Cephalo-Phi-3-MoE-vision-128k-3x4b-beta\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_name_moe, trust_remote_code=True)\n",
        "moe_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name_moe,\n",
        "    trust_remote_code=True,  torch_dtype=torch.bfloat16,\n",
        ").to(device)\n",
        "count_parameters(moe_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8HaiFHamzCt"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi, hf_hub_download\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Repository details\n",
        "repo_id = \"lamm-mit/Cephalo-Phi-3-MoE-vision-128k-3x4b-beta\"\n",
        "api = HfApi()\n",
        "\n",
        "# List all files in the repository\n",
        "files_in_repo = api.list_repo_files(repo_id)\n",
        "\n",
        "# Filter for .py files\n",
        "py_files = [file for file in files_in_repo if file.endswith('.py')]\n",
        "\n",
        "# Directory to save the downloaded files\n",
        "save_dir = \"./Phi_3V_MoE/\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Download each .py file\n",
        "for file_name in tqdm(py_files):\n",
        "    file_path = hf_hub_download(repo_id=repo_id, filename=file_name)\n",
        "    new_path = os.path.join(save_dir, file_name)\n",
        "    shutil.move(file_path, new_path)\n",
        "    print(f\"Downloaded: {file_name}\")\n",
        "\n",
        "print(\"Download completed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t94WWWDTm3MU"
      },
      "outputs": [],
      "source": [
        "from Phi_3V_MoE.moe_phi3_v import Phi3VForCausalLMMoE, Phi3VForCausalLMMoEConfig\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#Model specialized in bio-inspired/mechanics and materials\n",
        "model_name_1 = f\"lamm-mit/Cephalo-Phi-3-vision-128k-4b-beta\"\n",
        "model_1 = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name_1,\n",
        "    trust_remote_code=True,  torch_dtype=torch.bfloat16,\n",
        "\n",
        ").to(device)\n",
        "\n",
        "#Original model\n",
        "model_name_2 = f\"microsoft/Phi-3-vision-128k-instruct\"\n",
        "model_2 = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name_2,\n",
        "    trust_remote_code=True,  torch_dtype=torch.bfloat16,\n",
        "\n",
        ").to(device)\n",
        "\n",
        "#Model trained on conversion of images to LaTeX formulas\n",
        "model_name_3 = f\"lamm-mit/Cephalo-LaTeX-Phi-3-vision-128k-4b-beta\"\n",
        "model_3 = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name_3,\n",
        "    trust_remote_code=True,  torch_dtype=torch.bfloat16,\n",
        "\n",
        ").to(device)\n",
        "\n",
        "dtype = torch.bfloat16  # Desired dtype for new layers in MoE model\n",
        "\n",
        "# Initialize the models\n",
        "base_model = copy.deepcopy(model_2)  # Your base model\n",
        "expert_models = [model_1, model_2,  model_3  ]  # List of expert models\n",
        "\n",
        "# Load a processor (e.g. from base model)\n",
        "processor = AutoProcessor.from_pretrained(model_name_2, trust_remote_code=True)\n",
        "\n",
        "# Create the config\n",
        "config =  AutoConfig.from_pretrained(model_name_2, trust_remote_code=True)\n",
        "\n",
        "# Create the MoE model\n",
        "moe_config = Phi3VForCausalLMMoEConfig(config=config, k=1, num_expert_models=len (expert_models))\n",
        "moe_model = Phi3VForCausalLMMoE(moe_config, base_model, expert_models,  layer_dtype = dtype).to(device)\n",
        "\n",
        "count_parameters(expert_models[0]),count_parameters(moe_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSlDT-s9q1KO"
      },
      "outputs": [],
      "source": [
        "messages = [ {\"role\": \"user\", \"content\": \"<|image_1|>\\nWhat is shown in this image, and what is the relevance for materials design?\"}, ]\n",
        "prompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsDbwt05q31d"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "image_1 = Image.open(requests.get(\"https://d2r55xnwy6nx47.cloudfront.net/uploads/2018/02/Ants_Lede1300.jpg\", stream=True).raw)\n",
        "image_2 = Image.open(requests.get(\"https://https://images.pexels.com/photos/106399/pexels-photo-106399.jpeg\", stream=True).raw)\n",
        "image_3 = Image.open(requests.get(\"https://upload.wikimedia.org/wikipedia/commons/a/a0/Euplectella_aspergillum_Okeanos.jpg\", stream=True).raw)\n",
        "\n",
        "prompts_per_expert = [\n",
        "    [{\"text\": \"<|user|>\\n<|image_1|>\\nPrompt 1 for expert 1<|end|>\\n<|assistant|>\\n\", \"image\": [image_1]},\n",
        "     {\"text\": \"<|user|>\\n<|image_1|>\\nPrompt 2 for expert 1<|end|>\\n<|assistant|>\\n\", \"image\": [image_1]}],\n",
        "\n",
        "    [{\"text\": \"<|user|>\\n<|image_1|>\\nPrompt 1 for expert 2<|end|>\\n<|assistant|>\\n\", \"image\": [image_2]},\n",
        "     {\"text\": \"<|user|>\\n<|image_1|>\\nPrompt 2 for expert 2<|end|>\\n<|assistant|>\\n\", \"image\": [image_2]}],\n",
        "\n",
        "    [{\"text\": \"<|user|>\\n<|image_1|>\\nPrompt 1 for expert 3<|end|>\\n<|assistant|>\\n\", \"image\": [image_3]},\n",
        "     {\"text\": \"<|user|>\\n<|image_1|>\\nPrompt 2 for expert 3<|end|>\\n<|assistant|>\\n\", \"image\": [image_3]}],\n",
        "]\n",
        "\n",
        "# Train gating layers using the provided prompts\n",
        "gating_layer_params = moe_model.train_gating_layer_params_from_hidden_states(processor, prompts_per_expert,\n",
        "                                              epochs=1000,\n",
        "                                              loss_steps=100,\n",
        "                                              lr=5e-5,\n",
        "                                          )\n",
        "\n",
        "# Set parameters\n",
        "moe_model.set_gating_layer_params(gating_layer_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUV7RjzirVDF"
      },
      "outputs": [],
      "source": [
        "freeze_except_gating_layers(moe_model)\n",
        "count_parameters(moe_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBwop12drXBV"
      },
      "outputs": [],
      "source": [
        "un_freeze_all(moe_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iF4_G3LraTt"
      },
      "outputs": [],
      "source": [
        "FT_repo_id='xxxxx/' #<repo_ID>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWwuwtSk205-"
      },
      "source": [
        "**Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kusQAVglrcJ0"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "train_dataset = load_dataset(\"lamm-mit/Cephalo-Wikipedia-Materials\", split=\"train\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxBijYRcrfOd"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class MyDataCollator:\n",
        "    def __init__(self, processor):\n",
        "        self.processor = processor\n",
        "\n",
        "    def __call__(self, examples):\n",
        "        texts = []\n",
        "        images = []\n",
        "        for example in examples:\n",
        "            image = example[\"image\"]\n",
        "            question = example[\"query\"]\n",
        "            answer = example[\"answer\"]\n",
        "            messages = [ {\n",
        "                            \"role\": \"user\",  \"content\": '<|image_1|>\\n'+question},\n",
        "                           {\"role\": \"assistant\", \"content\": f\"{answer}\"}, ]\n",
        "\n",
        "            text = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
        "\n",
        "            images.append(image)\n",
        "\n",
        "        batch = processor(text=text, images=[image], return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        labels = batch[\"input_ids\"].clone()\n",
        "        labels[labels <0] = -100\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n",
        "\n",
        "data_collator = MyDataCollator(processor)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA6rEZm73A42"
      },
      "source": [
        "**Training Agrs and Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydmP7ahSrhVd"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "optim = \"paged_adamw_8bit\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=250,\n",
        "    learning_rate=1e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=25,\n",
        "    output_dir=\"output_training\",\n",
        "    optim=optim,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=1000,\n",
        "    save_total_limit=16,\n",
        "    #fp16=True,\n",
        "    bf16=True,\n",
        "    push_to_hub_model_id=FT_repo_id,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=moe_model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNGnjvbPrjte"
      },
      "outputs": [],
      "source": [
        "merged_name='Cephalo-Phi-3-MoE-vision-128k-3x4b'\n",
        "repo_id= '...'\n",
        "processor.push_to_hub (repo_id+'/'+merged_name, safe_serialization=False)\n",
        "moe_model.push_to_hub (repo_id+'/'+merged_name, safe_serialization=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "096TstnOrlcF"
      },
      "outputs": [],
      "source": [
        "merged_name='Cephalo-Phi-3-MoE-vision-128k-3x4b'\n",
        "processor.save_pretrained(merged_name,safe_serialization=False)\n",
        "moe_model.save_pretrained(merged_name,safe_serialization=False )\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
