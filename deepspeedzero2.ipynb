{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kN_iEa33Zmh7"
   },
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install -e git+https://github.com/OpenAccess-AI-Collective/axolotl#egg=axolotl\n",
    "!pip install flash-attn\n",
    "!pip install deepspeed\n",
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 764,
     "status": "ok",
     "timestamp": 1717693524034,
     "user": {
      "displayName": "Wasif Mehmood",
      "userId": "12083044002541848681"
     },
     "user_tz": -300
    },
    "id": "S4n4qK_tZqz3"
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Your YAML string\n",
    "yaml_string = \"\"\"\n",
    "base_model: meta-llama/Meta-Llama-3-8B-Instruct\n",
    "model_type: AutoModelForCausalLM\n",
    "tokenizer_type: AutoTokenizer\n",
    "\n",
    "load_in_8bit: false\n",
    "load_in_4bit: true\n",
    "strict: false\n",
    "\n",
    "datasets:\n",
    "  - path: medalpaca/medical_meadow_medqa\n",
    "    type: alpaca\n",
    "dataset_prepared_path:\n",
    "val_set_size: 0.1\n",
    "output_dir: ./outputs\n",
    "\n",
    "adapter: qlora\n",
    "lora_model_dir:\n",
    "\n",
    "sequence_len: 4096\n",
    "sample_packing: true\n",
    "pad_to_sequence_len: true\n",
    "\n",
    "lora_r: 32\n",
    "lora_alpha: 16\n",
    "lora_dropout: 0.05\n",
    "lora_target_modules:\n",
    "    - layers.26.self_attn.v_proj\n",
    "    - layers.17.self_attn.v_proj\n",
    "    - layers.28.self_attn.v_proj\n",
    "    - layers.3.self_attn.v_proj\n",
    "    - layers.29.self_attn.v_proj\n",
    "    - layers.21.self_attn.v_proj\n",
    "    - layers.4.mlp.up_proj\n",
    "    - layers.0.mlp.up_proj\n",
    "    - layers.1.mlp.down_proj\n",
    "    - layers.3.mlp.up_proj\n",
    "    - layers.16.self_attn.v_proj\n",
    "    - layers.15.self_attn.v_proj\n",
    "    - layers.0.mlp.down_proj\n",
    "    - layers.20.self_attn.v_proj\n",
    "    - layers.25.self_attn.v_proj\n",
    "    - layers.5.mlp.up_proj\n",
    "    - layers.7.mlp.up_proj\n",
    "lora_target_linear: true\n",
    "lora_fan_in_fan_out:\n",
    "\n",
    "wandb_project: medsft\n",
    "wandb_entity:\n",
    "wandb_watch:\n",
    "wandb_name:\n",
    "wandb_log_model:\n",
    "\n",
    "gradient_accumulation_steps: 8\n",
    "micro_batch_size: 2\n",
    "num_epochs: 2\n",
    "optimizer: paged_adamw_32bit\n",
    "lr_scheduler: cosine\n",
    "learning_rate: 0.00002\n",
    "\n",
    "train_on_inputs: false\n",
    "group_by_length: false\n",
    "bf16: auto\n",
    "fp16:\n",
    "tf32: false\n",
    "\n",
    "gradient_checkpointing: true\n",
    "early_stopping_patience:\n",
    "resume_from_checkpoint:\n",
    "local_rank:\n",
    "logging_steps: 1\n",
    "xformers_attention:\n",
    "flash_attention: true\n",
    "\n",
    "warmup_steps: 10\n",
    "evals_per_epoch: 2\n",
    "eval_table_size:\n",
    "saves_per_epoch:\n",
    "save_strategy:\n",
    "save_steps:\n",
    "debug:\n",
    "deepspeed: deepspeed_configs/zero2.json\n",
    "weight_decay: 0.0\n",
    "fsdp:\n",
    "fsdp_config:\n",
    "special_tokens:\n",
    "  pad_token: \"<|end_of_text|>\"\n",
    "\"\"\"\n",
    "\n",
    "# Convert the YAML string to a Python dictionary\n",
    "yaml_dict = yaml.safe_load(yaml_string)\n",
    "\n",
    "# Specify your file path\n",
    "file_path = 'multigpu.yaml'\n",
    "\n",
    "# Write the YAML file\n",
    "with open(file_path, 'w') as file:\n",
    "    yaml.dump(yaml_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7ONumk8qTJG"
   },
   "source": [
    "**Axolotl command for starting training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ku83TnqnaDxa"
   },
   "outputs": [],
   "source": [
    "!accelerate launch -m axolotl.cli.train multigpu.yaml"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOO7r++Wxr64obCYEF10JqC",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
