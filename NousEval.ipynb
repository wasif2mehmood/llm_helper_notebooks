{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETUP\n",
    "!git clone -b add-agieval https://github.com/dmahan93/lm-evaluation-harness\n",
    "%cd lm-evaluation-harness\n",
    "!pip install -e .\n",
    "\n",
    "#CONFIGURATION\n",
    "export MODEL=\"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "export TRUST_REMOTE_CODE=True\n",
    "\n",
    "#AGIEVAL\n",
    "!python main.py \\\n",
    "    --model hf-causal \\\n",
    "    --model_args pretrained=$MODEL,trust_remote_code=$TRUST_REMOTE_CODE \\\n",
    "    --tasks agieval_aqua_rat,agieval_logiqa_en,agieval_lsat_ar,agieval_lsat_lr,agieval_lsat_rc,agieval_sat_en,agieval_sat_en_without_passage,agieval_sat_math \\\n",
    "    --device cuda:0 \\\n",
    "    --batch_size auto \\\n",
    "    --output_path ./{\"agieval\"}.json\n",
    "\n",
    "#GPT4ALL\n",
    "!python main.py \\\n",
    "    --model hf-causal \\\n",
    "    --model_args pretrained=$MODEL,trust_remote_code=$TRUST_REMOTE_CODE \\\n",
    "    --tasks hellaswag,openbookqa,winogrande,arc_easy,arc_challenge,boolq,piqa \\\n",
    "    --device cuda:0 \\\n",
    "    --batch_size auto \\\n",
    "    --output_path ./{\"gpt4all\"}.json\n",
    "\n",
    "#TRUTHFULQA\n",
    "!python main.py \\\n",
    "    --model hf-causal \\\n",
    "    --model_args pretrained=$MODEL,trust_remote_code=$TRUST_REMOTE_CODE \\\n",
    "    --tasks truthfulqa_mc \\\n",
    "    --device cuda:0 \\\n",
    "    --batch_size auto \\\n",
    "    --output_path ./{\"truthfulqa\"}.json\n",
    "\n",
    "#BIGBENCH\n",
    "!python main.py \\\n",
    "    --model hf-causal \\\n",
    "    --model_args pretrained=$MODEL,trust_remote_code=True \\\n",
    "    --tasks bigbench_causal_judgement,bigbench_date_understanding,bigbench_disambiguation_qa,bigbench_geometric_shapes,bigbench_logical_deduction_five_objects,bigbench_logical_deduction_seven_objects,bigbench_logical_deduction_three_objects,bigbench_movie_recommendation,bigbench_navigate,bigbench_reasoning_about_colored_objects,bigbench_ruin_names,bigbench_salient_translation_error_detection,bigbench_snarks,bigbench_sports_understanding,bigbench_temporal_sequences,bigbench_tracking_shuffled_objects_five_objects,bigbench_tracking_shuffled_objects_seven_objects,bigbench_tracking_shuffled_objects_three_objects \\\n",
    "    --device cuda:0 \\\n",
    "    --batch_size auto \\\n",
    "    --output_path ./{\"bigbench\"}.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
