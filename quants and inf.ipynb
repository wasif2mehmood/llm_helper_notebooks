{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cd2e45-6cc7-4e2a-8496-32a6075a1c2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install optimum auto-gptq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452ef233-bdaf-42c9-9c1e-551055e37101",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token ''  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2352fb-9a93-40a7-b47f-db6c69986aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69152c49-9045-48c4-b839-26f22b0ba370",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "import time\n",
    "from auto_gptq import exllama_set_max_input_length\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_id = \"wasifis/Llama-3-8B-4bits\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model = exllama_set_max_input_length(model, max_input_length=2088)\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('iunput.csv')\n",
    "\n",
    "# Function to run inference on a prompt\n",
    "def generate_response(prompt):\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"system\", \"content\": \"You are a helpful medical assistant\"},\n",
    "         {\"role\": \"user\", \"content\": prompt}],\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(**inputs, do_sample=True, max_new_tokens=1000)\n",
    "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # Extract the response after the word \"assistant\"\n",
    "    assistant_keyword = \"assistant\"\n",
    "    response = response.split(assistant_keyword, 1)[1].strip()\n",
    "    \n",
    "    # Calculate the number of tokens generated\n",
    "    num_tokens = len(tokenizer.encode(response, add_special_tokens=False))\n",
    "    \n",
    "    print(response)\n",
    "    print(num_tokens)\n",
    "    return response, num_tokens\n",
    "\n",
    "# Measure the total inference time\n",
    "total_start_time = time.time()\n",
    "\n",
    "# Initialize total tokens counter\n",
    "total_tokens_generated = 0\n",
    "\n",
    "# Apply the function to each prompt in the 'summ_prmpt' column\n",
    "for index, row in df.iterrows():\n",
    "    response, num_tokens = generate_response(row['summ_prmpt'])\n",
    "    df.at[index, 'response'] = response\n",
    "    total_tokens_generated += num_tokens\n",
    "\n",
    "# Measure the total inference time\n",
    "total_end_time = time.time()\n",
    "total_inference_time = total_end_time - total_start_time\n",
    "\n",
    "# Save the DataFrame to a new Excel file once after all responses are generated\n",
    "df.to_excel('output.xlsx', index=False)\n",
    "\n",
    "# Print the total inference time and total tokens generated\n",
    "print(f\"Total Inference Time: {total_inference_time:.2f} seconds\")\n",
    "print(f\"Total Tokens Generated: {total_tokens_generated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8b8e6c-83c6-4a61-bcc5-63d234343893",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "import torch\n",
    "\n",
    "model_id = \"wasifis/Llama-3-8B-v17\"\n",
    "\n",
    "num_samples = 756\n",
    "max_seq_len = 4064\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "examples = [\n",
    "    tokenizer(\n",
    "        \"auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "]\n",
    "\n",
    "quantize_config = BaseQuantizeConfig(\n",
    "  bits=8,\n",
    "  group_size=128,\n",
    "  desc_act=True,\n",
    "  model_file_base_name=\"model\",\n",
    "  damp_percent=0.1,\n",
    ")\n",
    "\n",
    "# Determine the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model and move it to the device\n",
    "model = AutoGPTQForCausalLM.from_pretrained(\n",
    "  model_id,\n",
    "  quantize_config,\n",
    "  device_map=\"auto\",\n",
    ").to(device)\n",
    "\n",
    "# Ensure all tensors in examples are on the same device\n",
    "for example in examples:\n",
    "    for key in example:\n",
    "        example[key] = example[key].to(device)\n",
    "\n",
    "model.quantize(examples)\n",
    "model.save_quantized(\"Meta\", use_safetensors=True)\n",
    "tokenizer.save_pretrained(\"Meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a98ffa-6526-4649-9236-f14ac2eaa3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd Meta\n",
    "!huggingface-cli upload wasifis/Llama-3-8B-v17-8bits ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be599648-1be3-4ea4-af51-bdd1b50d1b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_excel('output.xlsx')\n",
    "\n",
    "# Function to extract text after the word \"assistant\"\n",
    "def extract_response(text):\n",
    "    assistant_keyword = \"assistant\"\n",
    "    if assistant_keyword in text:\n",
    "        return text.split(assistant_keyword, 1)[1].strip()\n",
    "    return text\n",
    "\n",
    "# Apply the function to the 'responses' column\n",
    "df['response'] = df['response'].apply(extract_response)\n",
    "\n",
    "# Save the modified DataFrame to a new Excel file\n",
    "df.to_excel('llamav174bitsoutput.xlsx', index=False)\n",
    "\n",
    "print(\"Processing complete. The modified Excel file has been saved as 'output.xlsx'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc076770-f9bd-4d74-b297-974e8affecf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
